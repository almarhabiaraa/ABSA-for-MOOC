{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SOJFlqWAUdvY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from rake_nltk import Rake\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Load preprocessed data\n",
        "df = pd.read_csv(\"cleaned_text.csv\")\n",
        "reviews = df['cleaned_text'].dropna().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJFenIZ_UpXp"
      },
      "source": [
        "# Step 1: Dependency Parsing to Extract Nouns & Noun Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9pONHV9CUdx_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 519886/519886 [07:45<00:00, 1116.34it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "texts = df['cleaned_text'].astype(str).fillna(\"\").tolist()\n",
        "noun_phrases_list = []\n",
        "\n",
        "for doc in tqdm(nlp.pipe(texts, batch_size=64), total=len(texts)):\n",
        "    noun_phrases = [\n",
        "        chunk.lemma_.lower()\n",
        "        for chunk in doc.noun_chunks\n",
        "        if 1 <= len(chunk.text.split()) <= 3\n",
        "    ]\n",
        "    noun_phrases_list.append(noun_phrases)\n",
        "\n",
        "df['noun_phrases'] = noun_phrases_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXDwQJqeUwIv"
      },
      "source": [
        "# Step 2: RAKE Keyword Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kPFeg-mnUd0l"
      },
      "outputs": [],
      "source": [
        "# Handle missing values by replacing NaNs with empty strings\n",
        "df['cleaned_text'] = df['cleaned_text'].fillna(\"\")\n",
        "\n",
        "# Now run the extraction using ThreadPoolExecutor\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Define the function to extract RAKE keywords\n",
        "def extract_rake_keywords(text):\n",
        "    rake = Rake()\n",
        "    rake.extract_keywords_from_text(text)\n",
        "    return rake.get_ranked_phrases()\n",
        "\n",
        "# Process the text in parallel\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    df['rake_keywords'] = list(executor.map(extract_rake_keywords, df['cleaned_text']))\n",
        "\n",
        "# Combine noun phrases and RAKE keywords\n",
        "df['aspect_candidates'] = df.apply(\n",
        "    lambda row: list(set(row['noun_phrases'] + row['rake_keywords'])), axis=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amjGABcMU8Ot"
      },
      "source": [
        "# Step 3: Train Word2Vec Model for Similarity Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FEZ_b1bqUd3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Aspect Cluster: content\n",
            " - material (0.79)\n",
            " - information (0.61)\n",
            " - syllabus (0.59)\n",
            " - lecture (0.55)\n",
            " - topic (0.53)\n",
            "\n",
            "Aspect Cluster: video\n",
            " - lecture (0.73)\n",
            " - slide (0.7)\n",
            " - clip (0.63)\n",
            " - transcript (0.62)\n",
            " - podcast (0.61)\n",
            "\n",
            "Aspect Cluster: quiz\n",
            " - test (0.76)\n",
            " - quizz (0.74)\n",
            " - exam (0.73)\n",
            " - homework (0.69)\n",
            " - assessment (0.68)\n",
            "\n",
            "Aspect Cluster: assignment\n",
            " - exercise (0.85)\n",
            " - assigment (0.83)\n",
            " - homework (0.82)\n",
            " - assessment (0.76)\n",
            " - assignement (0.75)\n",
            "\n",
            "Aspect Cluster: teacher\n",
            " - instructor (0.83)\n",
            " - professor (0.82)\n",
            " - lecturer (0.8)\n",
            " - tutor (0.77)\n",
            " - prof (0.66)\n",
            "\n",
            "Aspect Cluster: platform\n",
            " - site (0.58)\n",
            " - opportunity (0.58)\n",
            " - app (0.55)\n",
            " - service (0.53)\n",
            " - environment (0.53)\n",
            "\n",
            "Aspect Cluster: course\n",
            " - class (0.65)\n",
            " - one (0.54)\n",
            " - program (0.46)\n",
            " - believe (0.44)\n",
            " - necessary (0.43)\n",
            "\n",
            "Aspect Cluster: usability\n",
            " - modernday (0.82)\n",
            " - uso (0.82)\n",
            " - microcourse (0.82)\n",
            " - weekin (0.81)\n",
            " - dictatorship (0.81)\n"
          ]
        }
      ],
      "source": [
        "# Tokenize sentences\n",
        "tokenized_reviews = [word_tokenize(text) for text in reviews]\n",
        "\n",
        "# Train Word2Vec model\n",
        "w2v_model = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "# Define seed aspects\n",
        "seed_aspects = [\"content\", \"video\", \"quiz\", \"assignment\", \"teacher\", \"platform\", \"course\", \"usability\"]\n",
        "\n",
        "# Expand seed aspects using similarity\n",
        "similar_terms = {}\n",
        "for word in seed_aspects:\n",
        "    if word in w2v_model.wv:\n",
        "        similar_terms[word] = w2v_model.wv.most_similar(word, topn=5)\n",
        "\n",
        "# Display expanded aspect clusters\n",
        "for k, v in similar_terms.items():\n",
        "    print(f\"\\nAspect Cluster: {k}\")\n",
        "    for term, score in v:\n",
        "        print(f\" - {term} ({round(score, 2)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGOrhp7eVCuE"
      },
      "source": [
        "# Step 4: Aspect Term Frequency & Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "k-VjSBshVM07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 20 Frequent Aspect Terms:\n",
            "course: 56931\n",
            "nan: 30697\n",
            "great course: 19505\n",
            "good course: 17644\n",
            "excellent course: 10834\n",
            "good: 10541\n",
            "lot: 9087\n",
            "thank: 7388\n",
            "people: 4711\n",
            "amazing course: 4244\n",
            "excellent: 4171\n",
            "concept: 3772\n",
            "nice course: 3597\n",
            "great: 3329\n",
            "knowledge: 2911\n",
            "love course: 2874\n",
            "love: 2822\n",
            "awesome course: 2626\n",
            "thing: 2544\n",
            "wonderful course: 2348\n"
          ]
        }
      ],
      "source": [
        "# Flatten all extracted aspect terms\n",
        "all_aspects = [aspect for sublist in df['aspect_candidates'] for aspect in sublist]\n",
        "aspect_counter = Counter(all_aspects)\n",
        "\n",
        "# Filter frequently mentioned aspects (threshold can be adjusted)\n",
        "common_aspects = {aspect: count for aspect, count in aspect_counter.items() if count >= 10}\n",
        "sorted_aspects = sorted(common_aspects.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\nTop 20 Frequent Aspect Terms:\")\n",
        "for aspect, freq in sorted_aspects[:20]:\n",
        "    print(f\"{aspect}: {freq}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 5: Combine and Save File "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top Frequent Aspect Terms:\n",
            "course: 56931\n",
            "nan: 30697\n",
            "great course: 19505\n",
            "good course: 17644\n",
            "excellent course: 10834\n",
            "good: 10541\n",
            "lot: 9087\n",
            "thank: 7388\n",
            "people: 4711\n",
            "amazing course: 4244\n",
            "excellent: 4171\n",
            "concept: 3772\n",
            "nice course: 3597\n",
            "great: 3329\n",
            "knowledge: 2911\n",
            "love course: 2874\n",
            "love: 2822\n",
            "awesome course: 2626\n",
            "thing: 2544\n",
            "wonderful course: 2348\n",
            "                                             reviews       reviewers  \\\n",
            "0  Pretty dry, but I was able to pass with just t...     By Robert S   \n",
            "1  would be a better experience if the video and ...  By Gabriel E R   \n",
            "2  Information was perfect! The program itself wa...      By Jacob D   \n",
            "3  A few grammatical mistakes on test made me do ...       By Dale B   \n",
            "4  Excellent course and the training provided was...       By Sean G   \n",
            "\n",
            "   date_reviews  rating                 course_id  \\\n",
            "0  Feb 12, 2020       4  google-cbrs-cpi-training   \n",
            "1  Sep 28, 2020       4  google-cbrs-cpi-training   \n",
            "2  Apr 08, 2020       4  google-cbrs-cpi-training   \n",
            "3  Feb 24, 2020       4  google-cbrs-cpi-training   \n",
            "4  Jun 18, 2020       4  google-cbrs-cpi-training   \n",
            "\n",
            "                                        cleaned_text  \\\n",
            "0  dry able pass complete watch happy usual quest...   \n",
            "1  well experience video screen shot sho side tex...   \n",
            "2  information perfect program little annoying wa...   \n",
            "3      grammatical mistake test make double take bad   \n",
            "4  excellent course training provide detailed eas...   \n",
            "\n",
            "                                        noun_phrases  \\\n",
            "0                                   [many test take]   \n",
            "1                         [video screen, user, text]   \n",
            "2                      [information perfect program]   \n",
            "3                         [grammatical mistake test]   \n",
            "4  [excellent course training, detailed easy follow]   \n",
            "\n",
            "                                       rake_keywords  \\\n",
            "0  [dry able pass complete watch happy usual ques...   \n",
            "1  [well experience video screen shot sho side te...   \n",
            "2  [information perfect program little annoying w...   \n",
            "3    [grammatical mistake test make double take bad]   \n",
            "4  [excellent course training provide detailed ea...   \n",
            "\n",
            "                                   aspect_candidates  \n",
            "0  [many test take, dry able pass complete watch ...  \n",
            "1  [text, well experience video screen shot sho s...  \n",
            "2  [information perfect program little annoying w...  \n",
            "3  [grammatical mistake test, grammatical mistake...  \n",
            "4  [detailed easy follow, excellent course traini...  \n"
          ]
        }
      ],
      "source": [
        "# After extracting and processing the aspects\n",
        "\n",
        "# Combine noun phrases and RAKE keywords\n",
        "df['aspect_candidates'] = df.apply(\n",
        "    lambda row: list(set(row['noun_phrases'] + row['rake_keywords'])), axis=1\n",
        ")\n",
        "\n",
        "# Optionally filter frequently mentioned aspects (e.g., threshold based on count)\n",
        "all_aspects = [aspect for sublist in df['aspect_candidates'] for aspect in sublist]\n",
        "aspect_counter = Counter(all_aspects)\n",
        "common_aspects = {aspect: count for aspect, count in aspect_counter.items() if count >= 10}\n",
        "sorted_aspects = sorted(common_aspects.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Optionally print the top aspects\n",
        "print(\"\\nTop Frequent Aspect Terms:\")\n",
        "for aspect, freq in sorted_aspects[:20]:\n",
        "    print(f\"{aspect}: {freq}\")\n",
        "\n",
        "# Save the final DataFrame containing aspect candidates to a CSV file\n",
        "df.to_csv(\"final_extracted_aspects.csv\", index=False)\n",
        "print(df.head())  # Check the first few rows of the dataframe"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sentiment_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
